{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "from keras.models import load_model\n",
    "from os.path import join as pathjoin\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras_unet.models import vanilla_unet\n",
    "from keras_unet.models import custom_unet\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from skimage import img_as_ubyte, img_as_bool\n",
    "import numpy as np\n",
    "from img_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Unet with membrane data\n",
    "membrane data is in folder membrane/, it is a binary classification task.\n",
    "\n",
    "The input shape of image and mask are the same :(batch_size,rows,cols,channel = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "grid_length = 256\n",
    "model_path=f'unet_{grid_length}.hdf5'\n",
    "# train_dir=r'D:\\unet\\data\\membrane\\train'\n",
    "train_dir=f'{grid_length}/train'\n",
    "test_dir=f'{grid_length}/test'\n",
    "steps=3\n",
    "test_steps=2\n",
    "log_dir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# model_checkpoint = ModelCheckpoint(f'unet_{grid_length}.hdf5', monitor='loss',verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(1,train_dir,'image','label',data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gen=flow_samples_from_directories(pathjoin(train_dir,'image'),pathjoin(train_dir,'label'),False)\n",
    "samples_gen_list=list(samples_gen)\n",
    "sample_numbers=[0,2,3,4,5,7]*1\n",
    "# sample_numbers=list(range(len(samples_gen_list)))\n",
    "steps=len(sample_numbers)\n",
    "samples_gen=(samples_gen_list[i] for i in sample_numbers)\n",
    "myGene= itertools.cycle(tuple_map(to_ndarray,star_batchify(samples_gen,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dieyepy\\src\\main\\python\\model.py:55: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  model = Model(input = inputs, output = conv10)\n"
     ]
    }
   ],
   "source": [
    "model = unet(lr=0.5e-4)\n",
    "input_shape=(256,256,1)\n",
    "# model = vanilla_unet(input_shape=input_shape)\n",
    "# model = custom_unet(\n",
    "#     input_shape,\n",
    "#     use_batch_norm=True,\n",
    "#     num_classes=1,\n",
    "#     filters=64,\n",
    "#     dropout=0.5,\n",
    "#     output_activation='sigmoid'\n",
    "# )\n",
    "# model.compile(\n",
    "#     optimizer=Adam(lr=1e-4), \n",
    "# #     optimizer=SGD(lr=0.01, momentum=0.99),\n",
    "#     loss='binary_crossentropy',\n",
    "#     #loss=jaccard_distance,\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam, SGD\n",
    "# from keras_unet.metrics import iou, iou_thresholded\n",
    "# from keras_unet.losses import jaccard_distance\n",
    "# model.compile(\n",
    "#     optimizer=Adam(1e-4), \n",
    "# #     optimizer=SGD(lr=0.01, momentum=0.99),\n",
    "#     loss='binary_crossentropy',\n",
    "#     #loss=jaccard_distance,\n",
    "#     metrics=['accuracy']\n",
    "# #     metrics=[iou, iou_thresholded]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(model_path, monitor='loss',verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGeneCopy,myGene=itertools.tee(myGene,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Found 8 images belonging to 1 classes.\n",
      "Found 8 images belonging to 1 classes.\n",
      "30/30 [==============================] - 189s 6s/step - loss: 0.6932 - accuracy: 0.5021\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.69319, saving model to unet_256.hdf5\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 184s 6s/step - loss: 0.6931 - accuracy: 0.4252\n",
      "\n",
      "Epoch 00002: loss improved from 0.69319 to 0.69312, saving model to unet_256.hdf5\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 186s 6s/step - loss: 0.9981 - accuracy: 0.4155\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.69312\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 190s 6s/step - loss: 0.6926 - accuracy: 0.5240\n",
      "\n",
      "Epoch 00004: loss improved from 0.69312 to 0.69256, saving model to unet_256.hdf5\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 190s 6s/step - loss: 0.6926 - accuracy: 0.5065\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.69256\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 188s 6s/step - loss: 0.6925 - accuracy: 0.5284\n",
      "\n",
      "Epoch 00006: loss improved from 0.69256 to 0.69252, saving model to unet_256.hdf5\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 177s 6s/step - loss: 0.6925 - accuracy: 0.5440\n",
      "\n",
      "Epoch 00007: loss improved from 0.69252 to 0.69252, saving model to unet_256.hdf5\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 157s 5s/step - loss: 0.6926 - accuracy: 0.5131\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.69252\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 157s 5s/step - loss: 0.6927 - accuracy: 0.4811\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.69252\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 158s 5s/step - loss: 0.6924 - accuracy: 0.5629\n",
      "\n",
      "Epoch 00010: loss improved from 0.69252 to 0.69238, saving model to unet_256.hdf5\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 157s 5s/step - loss: 0.6925 - accuracy: 0.5209\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.69238\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 174s 6s/step - loss: 0.6924 - accuracy: 0.5258\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.69238\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 160s 5s/step - loss: 0.6926 - accuracy: 0.4852\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.69238\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 158s 5s/step - loss: 0.6924 - accuracy: 0.5543\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.69238\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 157s 5s/step - loss: 0.6926 - accuracy: 0.5062\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.69238\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 157s 5s/step - loss: 0.6924 - accuracy: 0.5313\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.69238\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 170s 6s/step - loss: 0.6925 - accuracy: 0.5228\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.69238\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 197s 7s/step - loss: 0.6922 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00018: loss improved from 0.69238 to 0.69224, saving model to unet_256.hdf5\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 196s 7s/step - loss: 0.6924 - accuracy: 0.5212\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.69224\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 210s 7s/step - loss: 0.6924 - accuracy: 0.5195\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.69224\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 191s 6s/step - loss: 0.6922 - accuracy: 0.5356\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.69224\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 199s 7s/step - loss: 0.6924 - accuracy: 0.5293\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.69224\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 194s 6s/step - loss: 0.6926 - accuracy: 0.5073\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.69224\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 195s 7s/step - loss: 0.6923 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.69224\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 201s 7s/step - loss: 0.6923 - accuracy: 0.5265\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.69224\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 184s 6s/step - loss: 0.6923 - accuracy: 0.5241\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.69224\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 192s 6s/step - loss: 0.6923 - accuracy: 0.5334\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.69224\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 176s 6s/step - loss: 0.6923 - accuracy: 0.5104\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.69224\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 170s 6s/step - loss: 0.6922 - accuracy: 0.5092\n",
      "\n",
      "Epoch 00029: loss improved from 0.69224 to 0.69220, saving model to unet_256.hdf5\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 171s 6s/step - loss: 0.6925 - accuracy: 0.5506\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.69220\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 163s 5s/step - loss: 0.6920 - accuracy: 0.5194\n",
      "\n",
      "Epoch 00031: loss improved from 0.69220 to 0.69205, saving model to unet_256.hdf5\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 164s 5s/step - loss: 0.6913 - accuracy: 0.5117\n",
      "\n",
      "Epoch 00032: loss improved from 0.69205 to 0.69134, saving model to unet_256.hdf5\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 172s 6s/step - loss: 0.6934 - accuracy: 0.5127\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.69134\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 173s 6s/step - loss: 0.6924 - accuracy: 0.5356\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.69134\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 158s 5s/step - loss: 0.6925 - accuracy: 0.5026\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.69134\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 170s 6s/step - loss: 0.6924 - accuracy: 0.5397\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.69134\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 181s 6s/step - loss: 0.6923 - accuracy: 0.5498\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.69134\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 182s 6s/step - loss: 0.6923 - accuracy: 0.5211\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.69134\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 187s 6s/step - loss: 0.6925 - accuracy: 0.5033\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.69134\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 199s 7s/step - loss: 0.6925 - accuracy: 0.5205\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.69134\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 178s 6s/step - loss: 0.6922 - accuracy: 0.5467\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.69134\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 175s 6s/step - loss: 0.6922 - accuracy: 0.5279\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.69134\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 173s 6s/step - loss: 0.6926 - accuracy: 0.4756\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.69134\n",
      "Epoch 44/100\n",
      "25/30 [========================>.....] - ETA: 29s - loss: 0.6914 - accuracy: 0.6121"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(myGene,steps_per_epoch=30,epochs=100,callbacks=[model_checkpoint])\n",
    "# model.fit_generator(myGene,steps_per_epoch=steps,epochs=1,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test your model and save predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# best_model=unet(model_path)\n",
    "best_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save('1_overfit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def testGeneFromTrain():\n",
    "for i in itertools.islice(myGeneCopy,len(sample_numbers)):\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(np.squeeze(i[0]), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(np.squeeze(i[1]), cmap='gray', vmin=0, vmax=1)\n",
    "    \n",
    "    predict_label=best_model.predict(i[0])\n",
    "    print(predict_label.min(),predict_label.max(),predict_label.mean())\n",
    "    predict_label=np.squeeze(predict_label)\n",
    "    \n",
    "    plt.subplot(143)\n",
    "    plt.imshow(predict_label, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.subplot(144)\n",
    "    predict_label=(predict_label>=0.5).astype(float)\n",
    "    plt.imshow(predict_label, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "#     yield i[0]\n",
    "# testGene=testGeneFromTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# testGene =  map(to_ndarray,batchify(flow_test_samples_from_directory(os.path.join(train_dir,'image')),1))\n",
    "results = model.predict_generator(testGene,steps=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76937366 1.0 0.9999909\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(results):\n",
    "#     bin_img=(item>=0.5)*255\n",
    "#     bin_img_u8=bin_img.astype(np.ubyte)\n",
    "#     print(hist)\n",
    "    print(item.min(),item.max(),item.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0\n",
      "1.0 1.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x2160 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20,30), subplot_kw={'xticks': [], 'yticks': []})\n",
    "for plot_cell, (i, item) in zip(axs.flat,enumerate(results)):\n",
    "#     img=img>=0.5\n",
    "#     print(img)\n",
    "#     img=img_as_ubyte(img)\n",
    "#     img=img.astype(np.ubyte)\n",
    "    img=img_as_float(item>=0.5)\n",
    "    plot_cell.imshow(item[:,:,0], cmap='gray', vmin=0, vmax=1)\n",
    "    print(img.min(), img.max(), img.mean())\n",
    "    io.imsave(os.path.join(f'{grid_length}/test/label', f\"{i}.png\"), item, check_contrast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
